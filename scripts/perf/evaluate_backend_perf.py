#!/usr/bin/env python
from __future__ import annotations

import argparse
import json
import sys
from pathlib import Path
from typing import Any, Dict, List


REPO_ROOT = Path(__file__).resolve().parents[2]
BACKEND_ROOT = REPO_ROOT / "backend"
if str(BACKEND_ROOT) not in sys.path:
    sys.path.insert(0, str(BACKEND_ROOT))

from app.services.perf_baseline import evaluate_reports  # noqa: E402


def build_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(description="Evaluate backend perf reports against thresholds.")
    p.add_argument(
        "--reports",
        nargs="+",
        required=True,
        help="One or more perf report json files generated by run_backend_perf.py",
    )
    p.add_argument(
        "--thresholds",
        default=str((Path(__file__).resolve().parent / "thresholds.default.json")),
        help="Threshold definition json path.",
    )
    p.add_argument(
        "--allow-missing-scenarios",
        action="store_true",
        help="Treat scenarios without data as skipped instead of failed.",
    )
    p.add_argument("--output-json", default="", help="Optional output json path.")
    return p


def load_json(path: Path) -> Dict[str, Any]:
    return json.loads(path.read_text(encoding="utf-8"))


def print_evaluation(result: Dict[str, Any]) -> None:
    print("== Threshold Evaluation ==")
    print(f"overall_passed: {result.get('all_passed')}")
    for scenario in result.get("results", []):
        flag = "PASS" if scenario.get("passed") else "FAIL"
        print(f"\n[{flag}] {scenario.get('scenario')}")
        for check in scenario.get("checks", []):
            cflag = "PASS" if check.get("passed") else "FAIL"
            metric = check.get("metric")
            threshold = check.get("threshold", {})
            actual = check.get("actual", {})
            if check.get("reason") == "no_data":
                print(f"  - [{cflag}] {metric} no_data threshold={threshold}")
                continue
            print(
                "  - [{flag}] {metric} actual(min={amin:.4f}, max={amax:.4f}, avg={aavg:.4f}, n={n}) threshold={thr}".format(
                    flag=cflag,
                    metric=metric,
                    amin=actual.get("min", 0.0),
                    amax=actual.get("max", 0.0),
                    aavg=actual.get("avg", 0.0),
                    n=actual.get("count", 0),
                    thr=threshold,
                )
            )


def main() -> int:
    args = build_parser().parse_args()
    report_paths = [Path(item).resolve() for item in args.reports]
    threshold_path = Path(args.thresholds).resolve()

    for rp in report_paths:
        if not rp.exists():
            raise FileNotFoundError(f"report not found: {rp}")
    if not threshold_path.exists():
        raise FileNotFoundError(f"thresholds file not found: {threshold_path}")

    reports: List[Dict[str, Any]] = [load_json(p) for p in report_paths]
    thresholds = load_json(threshold_path)
    result = evaluate_reports(
        reports=reports,
        thresholds=thresholds,
        allow_missing_scenarios=bool(args.allow_missing_scenarios),
    )
    result["meta"] = {
        "reports": [str(p) for p in report_paths],
        "thresholds": str(threshold_path),
    }

    print_evaluation(result)

    if args.output_json:
        out = Path(args.output_json).resolve()
        out.parent.mkdir(parents=True, exist_ok=True)
        out.write_text(json.dumps(result, ensure_ascii=False, indent=2), encoding="utf-8")
        print(f"\n[report] wrote json: {out}")

    return 0 if result.get("all_passed") else 2


if __name__ == "__main__":
    raise SystemExit(main())
